{
    "bge-small-en-v1.5-ia3-us": {
        "method": "ia3",
        "model": "BAAI/bge-small-en-v1.5",
        "peak_gpu_memory_mb": 3191.81884765625,
        "training_time_seconds": 312.34611105918884,
        "num_samples": 65534,
        "throughput_samples_per_sec": 209.81212084814933,
        "storage_size_mb": 1.1000423431396484,
        "num_epochs": 1,
        "batch_size": 16,
        "gradient_accumulation_steps": 2,
        "effective_batch_size": 32,
        "learning_rate": 2e-05,
        "lora_r": null,
        "lora_alpha": null,
        "lora_dropout": null,
        "HitRate@10": 0.6649484536082474,
        "Precision@10": 0.261340206185567,
        "ETFs": 194,
        "HitRate@3": 0.5463917525773195,
        "Precision@3": 0.3195876288659795,
        "HitRate@5": 0.6082474226804123,
        "Precision@5": 0.2907216494845361
    },
    "bge-small-en-v1.5-lora-us": {
        "method": "lora",
        "model": "BAAI/bge-small-en-v1.5",
        "peak_gpu_memory_mb": 2141.80419921875,
        "training_time_seconds": 289.23383259773254,
        "num_samples": 65534,
        "throughput_samples_per_sec": 226.57791936514192,
        "storage_size_mb": 2.10308837890625,
        "num_epochs": 1,
        "batch_size": 16,
        "gradient_accumulation_steps": 2,
        "effective_batch_size": 32,
        "learning_rate": 2e-05,
        "lora_r": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.1,
        "HitRate@10": 0.8041237113402062,
        "Precision@10": 0.3422680412371135,
        "ETFs": 194,
        "HitRate@3": 0.6082474226804123,
        "Precision@3": 0.3642611683848796,
        "HitRate@5": 0.6855670103092784,
        "Precision@5": 0.35154639175257757
    },
    "bge-small-en-v1.5-qlora-us": {
        "method": "qlora",
        "model": "BAAI/bge-small-en-v1.5",
        "peak_gpu_memory_mb": 2356.80615234375,
        "training_time_seconds": 459.08833479881287,
        "num_samples": 65534,
        "throughput_samples_per_sec": 142.7481271740853,
        "storage_size_mb": 2.1018810272216797,
        "num_epochs": 1,
        "batch_size": 16,
        "gradient_accumulation_steps": 2,
        "effective_batch_size": 32,
        "learning_rate": 2e-05,
        "lora_r": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.1,
        "HitRate@10": 0.788659793814433,
        "Precision@10": 0.338659793814433,
        "ETFs": 194,
        "HitRate@3": 0.5670103092783505,
        "Precision@3": 0.3505154639175257,
        "HitRate@5": 0.6443298969072165,
        "Precision@5": 0.34123711340206203
    },
    "gte-Qwen2-1.5B-instruct-ia3-us": {
        "method": "ia3",
        "model": "Alibaba-NLP/gte-Qwen2-1.5B-instruct",
        "peak_gpu_memory_mb": 49761.2060546875,
        "training_time_seconds": 2728.666112422943,
        "num_samples": 65534,
        "throughput_samples_per_sec": 24.016862928608187,
        "storage_size_mb": 16.373528480529785,
        "num_epochs": 1,
        "batch_size": 16,
        "gradient_accumulation_steps": 2,
        "effective_batch_size": 32,
        "learning_rate": 2e-05,
        "lora_r": null,
        "lora_alpha": null,
        "lora_dropout": null,
        "HitRate@10": 0.7783505154639175,
        "Precision@10": 0.36082474226804107,
        "ETFs": 194,
        "HitRate@3": 0.5927835051546392,
        "Precision@3": 0.37113402061855677,
        "HitRate@5": 0.6804123711340206,
        "Precision@5": 0.37731958762886586
    },
    "gte-Qwen2-1.5B-instruct-lora-us": {
        "method": "lora",
        "model": "Alibaba-NLP/gte-Qwen2-1.5B-instruct",
        "peak_gpu_memory_mb": 40556.82861328125,
        "training_time_seconds": 2416.0067768096924,
        "num_samples": 65534,
        "throughput_samples_per_sec": 27.124923915377774,
        "storage_size_mb": 23.542410850524902,
        "num_epochs": 1,
        "batch_size": 16,
        "gradient_accumulation_steps": 2,
        "effective_batch_size": 32,
        "learning_rate": 2e-05,
        "lora_r": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.1,
        "HitRate@10": 0.8350515463917526,
        "Precision@10": 0.3891752577319588,
        "ETFs": 194,
        "HitRate@3": 0.6597938144329897,
        "Precision@3": 0.40721649484536077,
        "HitRate@5": 0.7628865979381443,
        "Precision@5": 0.3989690721649485
    },
    "gte-Qwen2-1.5B-instruct-qlora-us": {
        "method": "qlora",
        "model": "Alibaba-NLP/gte-Qwen2-1.5B-instruct",
        "peak_gpu_memory_mb": 44977.412109375,
        "training_time_seconds": 3212.3811950683594,
        "num_samples": 65534,
        "throughput_samples_per_sec": 20.400443166772256,
        "storage_size_mb": 23.542410850524902,
        "num_epochs": 1,
        "batch_size": 16,
        "gradient_accumulation_steps": 2,
        "effective_batch_size": 32,
        "learning_rate": 2e-05,
        "lora_r": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.1,
        "HitRate@10": 0.8195876288659794,
        "Precision@10": 0.38247422680412374,
        "ETFs": 194,
        "HitRate@3": 0.5876288659793815,
        "Precision@3": 0.3762886597938145,
        "HitRate@5": 0.6907216494845361,
        "Precision@5": 0.3835051546391751
    },
    "multilingual-e5-large-instruct-ia3-us": {
        "method": "ia3",
        "model": "intfloat/multilingual-e5-large-instruct",
        "peak_gpu_memory_mb": 13800.81591796875,
        "training_time_seconds": 1044.0885558128357,
        "num_samples": 65534,
        "throughput_samples_per_sec": 62.76670655486783,
        "storage_size_mb": 16.652554512023926,
        "num_epochs": 1,
        "batch_size": 16,
        "gradient_accumulation_steps": 2,
        "effective_batch_size": 32,
        "learning_rate": 2e-05,
        "lora_r": null,
        "lora_alpha": null,
        "lora_dropout": null,
        "ETFs": 194,
        "HitRate@10": 0.7371134020618557,
        "Precision@10": 0.32216494845360844,
        "HitRate@3": 0.5567010309278351,
        "Precision@3": 0.3419243986254296,
        "HitRate@5": 0.6494845360824743,
        "Precision@5": 0.34536082474226804
    },
    "multilingual-e5-large-instruct-lora-us": {
        "method": "lora",
        "model": "intfloat/multilingual-e5-large-instruct",
        "peak_gpu_memory_mb": 13251.3291015625,
        "training_time_seconds": 1054.4458463191986,
        "num_samples": 65534,
        "throughput_samples_per_sec": 62.15018080706797,
        "storage_size_mb": 22.375760078430176,
        "num_epochs": 1,
        "batch_size": 16,
        "gradient_accumulation_steps": 2,
        "effective_batch_size": 32,
        "learning_rate": 2e-05,
        "lora_r": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.1,
        "HitRate@10": 0.788659793814433,
        "Precision@10": 0.3190721649484538,
        "ETFs": 194,
        "HitRate@3": 0.5309278350515464,
        "Precision@3": 0.31099656357388333,
        "HitRate@5": 0.6752577319587629,
        "Precision@5": 0.3350515463917528
    },
    "multilingual-e5-large-instruct-qlora-us": {
        "method": "qlora",
        "model": "intfloat/multilingual-e5-large-instruct",
        "peak_gpu_memory_mb": 13063.50537109375,
        "training_time_seconds": 1380.666977405548,
        "num_samples": 65534,
        "throughput_samples_per_sec": 47.46546493285938,
        "storage_size_mb": 22.375760078430176,
        "num_epochs": 1,
        "batch_size": 16,
        "gradient_accumulation_steps": 2,
        "effective_batch_size": 32,
        "learning_rate": 2e-05,
        "lora_r": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.1,
        "HitRate@10": 0.7474226804123711,
        "Precision@10": 0.31804123711340215,
        "ETFs": 194,
        "HitRate@3": 0.5360824742268041,
        "Precision@3": 0.3298969072164949,
        "HitRate@5": 0.654639175257732,
        "Precision@5": 0.3309278350515464
    }
}