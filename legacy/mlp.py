#!/usr/bin/env python
# coding: utf-8
# pip install torch transformers datasets pandas tqdm sentence-transformers peft huggingface-hub


import os, torch, pandas as pd
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset
from tqdm import tqdm
from transformers import set_seed
from huggingface_hub import hf_hub_download
import matplotlib.pyplot as plt # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•˜ì´í¼íŒŒë¼ë¯¸í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SEED, BATCH, LR, WD        = 0, 16, 1e-4, 1e-6
EPOCHS, INPUT_D, TARGET_D  = 50, 60, 20
MARGIN                     = 0.1
PRECOMP_REPO, PRECOMP_FILE = "LUcowork/computed-emb", "precomputed_embs.pt"
DATASET_REPO, PRICE_PATH   = "LUcowork/stage1-rewritten-us-ticker", "./data/price.parquet"
TEST_REPO                  = "LUcowork/eval-us"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
set_seed(SEED)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1) í…ìŠ¤íŠ¸ ì„ë² ë”© ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
emb_path = hf_hub_download(PRECOMP_REPO, PRECOMP_FILE, repo_type="dataset")
emb_dict = torch.load(emb_path)  # { text : 1024-d Tensor }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2) ë°ì´í„° ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
raw_ds    = load_dataset(DATASET_REPO)               # train / valid
test_rows = load_dataset(TEST_REPO)["test"]          # test
price_df  = pd.read_parquet(PRICE_PATH).fillna(0.0).sort_index()

train_price = price_df.iloc[: INPUT_D + TARGET_D]    # í•™ìŠµìš©
eval_price  = price_df.iloc[-(INPUT_D + TARGET_D):]    # í‰ê°€ìš©

# ë³´ì¡° dict
hold2desc  = {ex["holding"]: ex["positive"]
              for split in ["train", "valid"] for ex in raw_ds[split]}
all_stocks = sorted(hold2desc)

# VALID : ì›ë¬¸ anchor â†’ ì •ë‹µ holdings
valid_set = {}
for ex in raw_ds["valid"]:
    valid_set.setdefault(ex["anchor"], set()).add(ex["holding"])

# TEST  : rewritten_etf_desc â†’ ì •ë‹µ holdings
test_set = {}
for r in test_rows:
    test_set.setdefault(r["rewritten_etf_desc"], set()).add(r["holding"])
print(f"Loaded test set: {len(test_set)} ETF anchors")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3) í‰ê°€ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def evaluate_valid(split_dict, stock_emb, stock_ret):
    """Top-K ìˆ˜ìµë¥ ë§Œ ì¶œë ¥ & dict ë°˜í™˜"""
    top_ret = {3: [], 5: [], 10: []}
    for a_txt in split_dict:
        vec = emb_dict.get(a_txt)
        if vec is None:
            continue
        sim  = F.cosine_similarity(vec.to(DEVICE).unsqueeze(0), stock_emb)
        rank = sim.argsort(descending=True)
        for k in (3,5,10):
            idx = rank[:k]
            top_ret[k].append(stock_ret[idx.cpu()].mean().item())

    print("â”€â”€ VALID ê²°ê³¼ (Returnë§Œ) â”€â”€")
    for k in (3,5,10):
        avg = sum(top_ret[k]) / len(top_ret[k])
        print(f"  K={k:<2} | Ret={avg:.4f}")
    return {k: sum(v)/len(v) for k,v in top_ret.items()}


def evaluate_test(split_dict, stock_emb, stock_ret):
    """Precision / Hitë§Œ ì¶œë ¥. top-5 return ë¦¬í„´(ì²´í¬í¬ì¸íŠ¸ìš©)"""
    prec, hit, ret = {k:[] for k in (3,5,10)}, {k:[] for k in (3,5,10)}, {k:[] for k in (3,5,10)}
    for a_txt, gold in split_dict.items():
        vec = emb_dict.get(a_txt)
        if vec is None:
            continue
        sim  = F.cosine_similarity(vec.to(DEVICE).unsqueeze(0), stock_emb)
        rank = sim.argsort(descending=True)
        for k in (3,5,10):
            idx     = rank[:k]
            preds   = [all_stocks[i] for i in idx]
            correct = sum(p in gold for p in preds)
            prec[k].append(correct/k)
            hit[k].append(float(correct>0))
            # ìˆ˜ìµë¥ ì€ ì²´í¬í¬ì¸íŠ¸ ì„ ì •ìš©ìœ¼ë¡œë§Œ ëª¨ì•„ì„œ
            ret[k].append(stock_ret[idx.cpu()].mean().item())

    # print("â”€â”€ TEST ê²°ê³¼ (P / Hit) â”€â”€")
    # for k in (3,5,10):
    #     p = sum(prec[k])/len(prec[k])
    #     h = sum(hit[k])/len(hit[k])
    #     print(f"  K={k:<2} | P={p:.4f} Hit={h:.4f}")
    # return sum(ret[5]) / len(ret[5])  
    all_k_results_dict = {}

    print("â”€â”€ TEST ê²°ê³¼ (P / Hit / Ret) â”€â”€") 
    for k in (3,5,10):
        # ê° ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í™•ì¸
        p = sum(prec[k])/len(prec[k]) if len(prec[k]) > 0 else 0.0
        h = sum(hit[k])/len(hit[k]) if len(hit[k]) > 0 else 0.0
        # Kë³„ í‰ê·  ìˆ˜ìµë¥  ê³„ì‚° ë° ì¶œë ¥ ì¶”ê°€
        r_avg = sum(ret[k])/len(ret[k]) if len(ret[k]) > 0 else 0.0
        print(f"  K={k:<2} | P={p:.4f} Hit={h:.4f} Ret={r_avg:.4f}") # Ret ì¶œë ¥ ì¶”ê°€
        
        all_k_results_dict[k] = {
            "precision": p,
            "hit": h,
            "avg_return": r_avg
        }
    
    return all_k_results_dict
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4) Stella-LoRA ë² ì´ìŠ¤ë¼ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with torch.no_grad():

    base_emb = torch.stack([emb_dict[hold2desc[t]] for t in all_stocks]).to(DEVICE)
    
    # í›ˆë ¨ ë£¨í”„ ë‚´ í‰ê°€ì—ì„œ ì‚¬ìš©ë  ë¯¸ë˜ ìˆ˜ìµë¥ ê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ê³„ì‚°
    # ì´ ìˆ˜ìµë¥ ì€ evaluate_testì˜ stock_ret ì¸ìë¡œ ì „ë‹¬ë˜ì–´ ë°˜í™˜ë˜ëŠ” top-5 ìˆ˜ìµë¥ ì„ ì˜ë¯¸ìˆê²Œ ë§Œë“¦
    actual_future_returns_for_eval = torch.tensor([
        eval_price[t].iloc[INPUT_D:INPUT_D+TARGET_D].sum()
        for t in all_stocks
    ])
    
    print("â”€â”€ BASELINE (Stella-LoRA) TEST ê²°ê³¼ (P / Hit) â”€â”€")
    # evaluate_test í•¨ìˆ˜ëŠ” P/Hitë¥¼ ì¶œë ¥í•˜ê³ , top-5 í‰ê·  ìˆ˜ìµë¥ ì„ ë°˜í™˜.
    baseline_results_dict = evaluate_test(test_set, base_emb, actual_future_returns_for_eval) # ë°˜í™˜ê°’ì€ ì‚¬ì „ì„
print("-"*60)

#     base_emb = torch.stack([emb_dict[hold2desc[t]] for t in all_stocks]).to(DEVICE)
#     zero_ret = torch.zeros(len(all_stocks))
#     _ = evaluate_test(test_set, base_emb, zero_ret)
# print("-"*60)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5) Dataset / DataLoader â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class TripletDS(Dataset):
    def __init__(self, split):
        self.rows = raw_ds[split]
        self.prc  = train_price
        self.h2d  = {ex["holding"]: ex["positive"] for ex in self.rows}
        anchors   = sorted({ex["anchor"] for ex in self.rows})
        self.a2i  = {a:i for i,a in enumerate(anchors)}
        self.bucket = {i:[] for i in range(len(anchors))}
        for ex in self.rows:
            self.bucket[self.a2i[ex["anchor"]]].append(ex["holding"])
    def __len__(self): return len(self.a2i)
    def __getitem__(self, i):
        a_txt = list(self.a2i.keys())[i]
        anc   = emb_dict[a_txt]
        lo, hi= self.bucket[i][0], self.bucket[i][-1]
        t_lo  = torch.tensor(self.prc[lo].iloc[:INPUT_D].values, dtype=torch.float32)
        t_hi  = torch.tensor(self.prc[hi].iloc[:INPUT_D].values, dtype=torch.float32)
        r_lo  = self.prc[lo].iloc[INPUT_D:INPUT_D+TARGET_D].sum()
        r_hi  = self.prc[hi].iloc[INPUT_D:INPUT_D+TARGET_D].sum()
        if r_hi >= r_lo:
            pos_ts, neg_ts = t_hi, t_lo
            pos_desc, neg_desc = self.h2d[hi], self.h2d[lo]
        else:
            pos_ts, neg_ts = t_lo, t_hi
            pos_desc, neg_desc = self.h2d[lo], self.h2d[hi]
        return dict(
            anc=anc,
            pos=emb_dict[pos_desc],
            neg=emb_dict[neg_desc],
            pts=torch.nan_to_num(pos_ts),
            nts=torch.nan_to_num(neg_ts),
        )

def collate(b): return {k:torch.stack([x[k] for x in b]) for k in b[0]}

train_loader = DataLoader(TripletDS("train"), BATCH, True, collate_fn=collate)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6) MLP ëª¨ë¸ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class TripletMLP(nn.Module):
    def __init__(self, td=1024, ts=INPUT_D):
        super().__init__()
        self.ts_mlp = nn.Sequential(
            nn.Linear(ts,256), nn.ReLU(), nn.Linear(256,td)
        )
    def forward(self, anc,pos,neg,pts,nts):
        pos_f = pos + self.ts_mlp(pts)
        neg_f = neg + self.ts_mlp(nts)
        dpos  = 1 - F.cosine_similarity(anc, pos_f)
        dneg  = 1 - F.cosine_similarity(anc, neg_f)
        return F.relu(dpos - dneg + MARGIN).mean()

model = TripletMLP().to(DEVICE)
opt   = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WD)
best_valid_avg, best_epoch = -float("inf"), -1

# ê·¸ë˜í”„ìš© ë°ì´í„° ì €ì¥
eval_epochs_history = []
test_metrics_history = {
    k_val: {'precision': [], 'hit': [], 'avg_return': []} for k_val in (3, 5, 10)
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7) í•™ìŠµ ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for ep in range(1, EPOCHS+1):
    # â”€â”€ Train
    model.train()
    total = 0.0
    for batch in tqdm(train_loader, desc=f"Epoch {ep} â–¶ Train"):
        batch = {k:v.to(DEVICE) for k,v in batch.items()}
        loss  = model(**batch)
        opt.zero_grad(); loss.backward(); opt.step()
        total += loss.item()
    print(f"Epoch {ep:3} â”‚ train loss {total/len(train_loader):.6f}")

    # â”€â”€ Eval (5 epië§ˆë‹¤)
    if ep % 5 == 0:
        model.eval()
        with torch.no_grad():
            stock_emb = torch.stack([emb_dict[hold2desc[t]] for t in all_stocks]).to(DEVICE)
            stock_ts  = torch.stack([
                torch.tensor(eval_price[t].iloc[:INPUT_D].values, dtype=torch.float32)
                for t in all_stocks
            ]).to(DEVICE)
            fused     = stock_emb + model.ts_mlp(stock_ts)
            future    = torch.tensor([
                eval_price[t].iloc[INPUT_D:INPUT_D+TARGET_D].sum()
                for t in all_stocks
            ])

            # VALID : returnë§Œ
            valid_ret_dict = evaluate_valid(valid_set, fused, future)
            valid_avg_ret  = sum(valid_ret_dict.values()) / 3.0

            
            # TEST  : P / Hit / Ret ì¶œë ¥ ë° ê²°ê³¼ ì €ì¥
            test_results_current_epoch = evaluate_test(test_set, fused, future)
            eval_epochs_history.append(ep)
            for k_val in (3, 5, 10):
                if k_val in test_results_current_epoch:
                    test_metrics_history[k_val]['precision'].append(test_results_current_epoch[k_val]['precision'])
                    test_metrics_history[k_val]['hit'].append(test_results_current_epoch[k_val]['hit'])
                    test_metrics_history[k_val]['avg_return'].append(test_results_current_epoch[k_val]['avg_return'])
                else: # í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš° ëŒ€ë¹„ (evaluate_testê°€ í•´ë‹¹ Kë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ì„ ë•Œ)
                    test_metrics_history[k_val]['precision'].append(0.0)
                    test_metrics_history[k_val]['hit'].append(0.0)
                    test_metrics_history[k_val]['avg_return'].append(0.0)
            print()

        # â”€â”€ ì²´í¬í¬ì¸íŠ¸ (VALID í‰ê·  return ê¸°ì¤€)
        if valid_avg_ret > best_valid_avg:
            best_valid_avg, best_epoch = valid_avg_ret, ep
            fn = f"triplet_mlp_best_ep{ep}.pt"
            torch.save(model.state_dict(), fn)
            print(f"ğŸ†  New BEST (VALID avg return {best_valid_avg:.4f}) â†’ {fn}\n")

print(f"ğŸ‰ í›ˆë ¨ ì¢…ë£Œ!  best_epoch={best_epoch}, best VALID avg return={best_valid_avg:.4f}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8) ê²°ê³¼ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if eval_epochs_history:
    metrics_to_plot = ['precision', 'hit', 'avg_return']
    ks_to_plot = [3, 5, 10]
    metric_display_names = {'precision': 'Precision', 'hit': 'Hit Rate', 'avg_return': 'Average Return'}
    # ê° K ê°’ì— ëŒ€í•œ ìƒ‰ìƒì„ ë¯¸ë¦¬ ì •ì˜ (ë² ì´ìŠ¤ë¼ì¸ê³¼ ì—í­ ê²°ê³¼ ìƒ‰ìƒ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•¨)
    k_colors = {3: 'blue', 5: 'green', 10: 'red'}


    num_metrics = len(metrics_to_plot)
    fig, axes = plt.subplots(num_metrics, 1, figsize=(12, 6 * num_metrics), sharex=True) # ë†’ì´ ì•½ê°„ ì¦ê°€
    if num_metrics == 1: 
        axes = [axes]

    for i, metric_key in enumerate(metrics_to_plot):
        ax = axes[i]
        for k_val in ks_to_plot:
            # ì—í­ë³„ ê²°ê³¼ í”Œë¡¯
            metric_values = test_metrics_history[k_val][metric_key]
            if len(metric_values) == len(eval_epochs_history):
                 ax.plot(eval_epochs_history, metric_values, label=f'K={k_val} (Epoch)', marker='o', linestyle='-', color=k_colors[k_val])
            else:
                 print(f"Warning: Mismatch in length for K={k_val}, Metric={metric_key}. Skipping epoch plot for this line.")
            
            # ë² ì´ìŠ¤ë¼ì¸ ê²°ê³¼ í”Œë¡¯ (ìˆ˜í‰ ì ì„ )
            if baseline_results_dict and k_val in baseline_results_dict and metric_key in baseline_results_dict[k_val]:
                baseline_val = baseline_results_dict[k_val][metric_key]
                ax.axhline(y=baseline_val, color=k_colors[k_val], linestyle='--', label=f'K={k_val} (Baseline)')
            else:
                print(f"Warning: Baseline data for K={k_val}, Metric={metric_key} not found. Skipping baseline plot.")


        ax.set_ylabel(metric_display_names[metric_key])
        ax.set_title(f'Test {metric_display_names[metric_key]} Over Epochs (vs Baseline)')
        # ë²”ë¡€ í•¸ë“¤ ë° ë¼ë²¨ì„ ê°€ì ¸ì™€ ì¤‘ë³µ ì œê±° í›„ í‘œì‹œ
        handles, labels = ax.get_legend_handles_labels()
        by_label = dict(zip(labels, handles)) # ìˆœì„œ ìœ ì§€ë¥¼ ìœ„í•´ OrderedDict ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŒ
        ax.legend(by_label.values(), by_label.keys())
        ax.grid(True)
    
    axes[-1].set_xlabel('Epoch')
    fig.suptitle('Test Metrics Evolution with Baseline Comparison', fontsize=16)
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    plt.savefig("test_metrics_evolution_with_baseline.png") # íŒŒì¼ëª… ë³€ê²½
else:
    print("No evaluation data to plot.")